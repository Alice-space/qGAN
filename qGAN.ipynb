{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Gauss distribution for clustering\n",
    "# mapping output of qNetwork to some distribution (measurement of probability)\n",
    "\n",
    "# Oh I need to read GAN, even try some diffusion tricks.\n",
    "# Target: make a Q-network to output a specific distribution? Why not Hamiltionian? \n",
    "\n",
    "# 1. make a bunch of data\n",
    "# 2. build a Q-network\n",
    "# 3. train the Q-network to output the distribution of the data using GAN\n",
    "# 4. test the Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014 0.068 0.187 0.29  0.258 0.133 0.037 0.013]\n",
      "Data generation done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# generation of data\n",
    "\n",
    "NumData = 1000\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "np.random.seed(520309)\n",
    "\n",
    "# make a Dataloader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GaussDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.hists = []\n",
    "    for _ in range(NumData):\n",
    "      data = np.random.normal(loc=0, scale=1, size=1000)\n",
    "      # Create histogram\n",
    "      hist, _ = np.histogram(data, bins=8)\n",
    "      # normalization\n",
    "      hist = hist / hist.sum()\n",
    "      self.hists.append(hist)\n",
    "  def __len__(self):\n",
    "    return len(self.hists)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.hists[idx]\n",
    "  \n",
    "dataset = GaussDataset()\n",
    "histdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Plot histogram\n",
    "# for i_batch, sample_batched in enumerate(histdataloader):\n",
    "#   print(i_batch, sample_batched.size())\n",
    "  # for i in range(5):\n",
    "  #   plt.plot(sample_batched[-i], 'o-')\n",
    "  # plt.xlabel('Bins')\n",
    "  # plt.ylabel('Frequency')\n",
    "  # plt.title('Gaussian Distribution')\n",
    "  # plt.show()\n",
    "\n",
    "print(dataset[-1])\n",
    "print('Data generation done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit done\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumRegister, QuantumCircuit\n",
    "from qiskit.circuit import QuantumCircuit, ParameterVector, Parameter\n",
    "\n",
    "qr = QuantumRegister(3, 'q')\n",
    "qc = QuantumCircuit(qr)\n",
    "\n",
    "# inputs = ParameterVector('inputs', 3)\n",
    "angles = ParameterVector('angles', 9)\n",
    "\n",
    "# qc.initialize(inputs[0], 0)\n",
    "\n",
    "# qc.ry(inputs[0], qr[0])\n",
    "# qc.ry(inputs[1], qr[1])\n",
    "# qc.ry(inputs[2], qr[2])\n",
    "\n",
    "# Layer 0\n",
    "qc.h(qr[0:3])\n",
    "qc.ry(angles[0], qr[0])\n",
    "qc.ry(angles[1], qr[1])\n",
    "qc.ry(angles[2], qr[2])\n",
    "\n",
    "# Layer 1\n",
    "\n",
    "qc.cx(qr[0], qr[1])\n",
    "qc.ry(angles[3], qr[0])\n",
    "qc.cx(qr[1], qr[2])\n",
    "qc.ry(angles[4], qr[1])\n",
    "qc.ry(angles[5], qr[2])\n",
    "\n",
    "# Layer 2\n",
    "qc.cx(qr[0], qr[1])\n",
    "qc.ry(angles[6], qr[0])\n",
    "qc.cx(qr[1], qr[2])\n",
    "qc.ry(angles[7], qr[1])\n",
    "qc.ry(angles[8], qr[2])\n",
    "\n",
    "# bind the parameters\n",
    "# qc = qc.bind_parameters({\n",
    "#   angles[0]: 0.5, angles[1]: 0.5, angles[2]: 0.5, angles[3]: 0.5, angles[4]: 0.5, angles[5]: 0.5, angles[6]: 0.5, angles[7]: 0.5, angles[8]: 0.5})\n",
    "\n",
    "# qc.measure_all()\n",
    "\n",
    "# qc.draw('mpl')\n",
    "print('Circuit done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a GAN discriminator\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear_input = nn.Linear(input_size, 20)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.linear20 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_input(input)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.linear20(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# pytorch QNN\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit_aer.primitives import Sampler\n",
    "\n",
    "# from qiskit.primitives import Sampler\n",
    "\n",
    "\n",
    "shots = 8192\n",
    "\n",
    "run_options = {\n",
    "    'shots': shots\n",
    "}\n",
    "\n",
    "# if cuda is available, use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "backend_options = {\n",
    "    'method': 'statevector',\n",
    "    'device': 'GPU' if device.type == 'cuda' else 'CPU',\n",
    "    'max_job_size': 100, # maximum number of circuits in one job\n",
    "    'max_parallel_threads': 1, # maximum number of CPU cores used by OpenMP for parallelization\n",
    "    'max_parallel_experiments': 1, # maximum number of experiments for parallelization\n",
    "}\n",
    "\n",
    "sampler = Sampler(run_options=run_options, backend_options=backend_options)\n",
    "\n",
    "\n",
    "def create_generator() -> TorchConnector:\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        sampler=sampler,\n",
    "        input_params=[],\n",
    "        weight_params=qc.parameters,\n",
    "        sparse=False,\n",
    "    )\n",
    "\n",
    "    initial_weights = algorithm_globals.random.random(qc.num_parameters)\n",
    "    return TorchConnector(qnn, initial_weights)\n",
    "\n",
    "\n",
    "generator = create_generator()\n",
    "discriminator = Discriminator(8)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.0002  # learning rate\n",
    "b1 = 0.5  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "\n",
    "generator_optimizer = Adam(generator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005)\n",
    "discriminator_optimizer = Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# gen_dist = generator(torch.tensor([[] for _ in range(batch_size)]))\n",
    "# print(gen_dist)\n",
    "\n",
    "print('GAN done')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/var/folders/07/0q8jfv9d173_9_jqgqxfv5zc0000gn/T/ipykernel_45233/3940514717.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  real_dist = torch.tensor(sample_batched, dtype=torch.float)\n",
      "/var/folders/07/0q8jfv9d173_9_jqgqxfv5zc0000gn/T/ipykernel_45233/3940514717.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  real_dist = torch.tensor(sample_batched, dtype=torch.float)\n",
      "  0%|          | 0/10000 [01:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(discriminator(gen_dist))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m generator_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(gen_dist), torch\u001b[38;5;241m.\u001b[39mones(sample_batched\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 57\u001b[0m \u001b[43mgenerator_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m generator_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# relative entropy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/qiskit_machine_learning/connectors/torch_connector.py:169\u001b[0m, in \u001b[0;36mTorchConnector._TorchNNFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     grad_output \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# evaluate QNN gradient\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m input_grad, weights_grad \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msparse:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/qiskit_machine_learning/neural_networks/neural_network.py:254\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m    252\u001b[0m input_, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(input_data)\n\u001b[1;32m    253\u001b[0m weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_weights(weights)\n\u001b[0;32m--> 254\u001b[0m input_grad, weight_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m input_grad_reshaped, weight_grad_reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_backward_output(\n\u001b[1;32m    257\u001b[0m     input_grad, weight_grad, shape\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_grad_reshaped, weight_grad_reshaped\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/qiskit_machine_learning/neural_networks/sampler_qnn.py:420\u001b[0m, in \u001b[0;36mSamplerQNN._backward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m QiskitMachineLearningError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampler job failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/site-packages/qiskit/primitives/primitive_job.py:55\u001b[0m, in \u001b[0;36mPrimitiveJob.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the results of the job.\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_submitted()\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qiskit/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "def relative_entropy(p, q):\n",
    "    # avoid nan and inf\n",
    "    p = torch.where(p <= 1e-6, torch.tensor(1e-6), p)\n",
    "    q = torch.where(q <= 1e-6, torch.tensor(1e-6), q)\n",
    "    return (p * torch.log(p / q)).sum()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "start = time.time()\n",
    "print('Start training')\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    generator_loss_values = []\n",
    "    discriminator_loss_values = []\n",
    "    entropy_values = []\n",
    "    for i_batch, sample_batched in enumerate(histdataloader):\n",
    "        # print(i_batch, sample_batched.size())\n",
    "        # update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # print(sample_batched.shape[0])\n",
    "        ## train with real\n",
    "        # sample_batched.to(device)\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_dist = torch.tensor(sample_batched, dtype=torch.float, device=device)\n",
    "\n",
    "        # real_dist = torch.where(real_dist <= 1e-6, torch.tensor(1e-6), real_dist)\n",
    "\n",
    "        discriminat_real_dist = discriminator(real_dist)\n",
    "        # print(\"real\", real_dist.shape, discriminat_real_dist.shape)\n",
    "        real_loss = criterion(discriminat_real_dist, torch.ones(sample_batched.shape[0], 1))\n",
    "        real_loss.backward()\n",
    "\n",
    "        ## train with fake\n",
    "        gen_dist = generator(torch.tensor([[] for _ in range(sample_batched.shape[0])]))\n",
    "        \n",
    "        # gen_dist = torch.where(gen_dist <= 1e-6, torch.tensor(1e-6), gen_dist)\n",
    "\n",
    "        discriminat_gen_dist = discriminator(gen_dist.detach())\n",
    "        # print(\"gen\", gen_dist.shape, discriminat_gen_dist.shape)\n",
    "        fake_loss = criterion(discriminat_gen_dist, torch.zeros(sample_batched.shape[0], 1))\n",
    "        fake_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # update G network: maximize log(D(G(z)))\n",
    "        generator_optimizer.zero_grad()\n",
    "        # print(discriminator(gen_dist))\n",
    "        generator_loss = criterion(discriminator(gen_dist), torch.ones(sample_batched.shape[0], 1))\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # relative entropy\n",
    "        mean_entropy = relative_entropy(real_dist, gen_dist.detach()).mean()\n",
    "        # print(f\"Epoch {epoch}, Batch {i_batch}, D Loss: {real_loss + fake_loss}, G Loss: {generator_loss}, Entropy: {mean_entropy}, real: {real_dist}, gen: {gen_dist}\")\n",
    "\n",
    "        # if np.isnan(mean_entropy) or np.isinf(mean_entropy):\n",
    "        #     print(f\"Epoch {epoch}, Batch {i_batch}, D Loss: {real_loss + fake_loss}, G Loss: {generator_loss}, Entropy: {entropy(real_dist, gen_dist.detach()).mean()}, real: {real_dist}, gen: {gen_dist}\")\n",
    "\n",
    "        batch_log_index = epoch * NumData / batch_size + i_batch\n",
    "                \n",
    "        writer.add_scalar('BatchLoss/DLoss', generator_loss.detach().item(), batch_log_index)\n",
    "        writer.add_scalar('BatchLoss/GLoss', real_loss.detach() + fake_loss.detach(), batch_log_index)\n",
    "        writer.add_scalar('Batch/Entropy', mean_entropy, batch_log_index)\n",
    "\n",
    "        generator_loss_values.append(generator_loss.detach().item())\n",
    "        discriminator_loss_values.append(real_loss.detach() + fake_loss.detach())\n",
    "        entropy_values.append(mean_entropy)\n",
    "        \n",
    "    writer.add_scalar('Loss/DLoss', np.mean(discriminator_loss_values), epoch)\n",
    "    writer.add_scalar('Loss/GLoss', np.mean(generator_loss_values), epoch)\n",
    "    writer.add_scalar('Entropy', np.mean(entropy_values), epoch)\n",
    "\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Fit in {elapsed:0.2f} sec\")\n",
    "\n",
    "# plot the result, hist graph for real_dist and line dot for gen_dist\n",
    "# real_dist, gen_dist\n",
    "# plt.bar(bins[:-1], real_dist.detach().numpy(), width=(bins[1]-bins[0]))\n",
    "# plt.plot(bins[:-1], gen_dist.detach().numpy(), 'r')\n",
    "# plt.show()\n",
    "# plt.bar(bins[:-1], real_dist.detach().numpy(), width=(bins[1]-bins[0]))\n",
    "# plt.bar(bins[:-1], gen_dist.detach().numpy(), width=(bins[1]-bins[0]))\n",
    "# plt.show()\n",
    "# print(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why we design the circuit in this way?\n",
    "\n",
    "# superposition + parameter ?\n",
    "\n",
    "# How to back propagate the parameter?\n",
    "\n",
    "# How to use the quantum feature to detect the minimum of the function?\n",
    "\n",
    "# Gedeer Encoding\n",
    "\n",
    "# EfficientSU2 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
